{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying Code from main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from robust_losses import RobustLoss\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_sizes, n_continuous, p_dropout = 0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding layers\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for num_embeddings, embedding_dim in embedding_sizes])\n",
    "        self.embeddings_dropout = nn.Dropout(p_dropout)\n",
    "        \n",
    "        # normalize continuous\n",
    "        self.normalize_continuous = nn.BatchNorm1d(n_continuous)\n",
    "\n",
    "        # linear FC layers\n",
    "        number_embeddings = sum([number_embeddings_per_column for _, number_embeddings_per_column in embedding_sizes])\n",
    "        in_features = number_embeddings + n_continuous\n",
    "        layer_list = [\n",
    "            nn.Linear(in_features=in_features, out_features=64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout),\n",
    "            nn.BatchNorm1d(64),\n",
    "\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout),\n",
    "            nn.BatchNorm1d(128),\n",
    "\n",
    "            nn.Linear(in_features=128, out_features=32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout),\n",
    "            nn.BatchNorm1d(32),\n",
    "            \n",
    "            nn.Linear(in_features=32, out_features=2),\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x_categorical, x_continuous):\n",
    "\n",
    "        x_categorical_tensor = x_categorical[0].int()\n",
    "        x_continuous_tensor = x_continuous[0]\n",
    "\n",
    "        # process embeddings\n",
    "        embeddings = []\n",
    "        for i, embedding_layer in enumerate(self.embeddings):\n",
    "            embeddings.append(embedding_layer(x_categorical_tensor[:, i]))\n",
    "        x_cat = torch.cat(embeddings, dim=1)\n",
    "        x_cat = self.embeddings_dropout(x_cat)\n",
    "\n",
    "        # process continuous\n",
    "        x_cont = self.normalize_continuous(x_continuous_tensor)\n",
    "\n",
    "        # concatenate all inputs\n",
    "        x = torch.cat([x_cat, x_cont], dim=1)\n",
    "\n",
    "        # apply layers\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testaccuracy(model, test_categorical_dataloader, test_continuous_dataloader, test_target_dataloader):\n",
    "    with torch.no_grad():\n",
    "        y_pred = None\n",
    "        y_test = None\n",
    "        for i, (x_cat, x_cont, y) in enumerate(zip(test_categorical_dataloader, test_continuous_dataloader, test_target_dataloader)):\n",
    "            y_pred = F.softmax(model(x_cat, x_cont))\n",
    "            y_test = y\n",
    "        print(y)\n",
    "        y_pred_class=y_pred.round()\n",
    "        accuracy=(y_pred_class.eq(y_test).sum())/float(y_test.shape[0])\n",
    "        return (accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model):\n",
    "    fname = \"DRO_model.pth\"\n",
    "    torch.save(model.state_dict(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/datasets/uci_adult/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthfols = glob(\"../data/datasets/uci_adult/synthetic/*/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [path]\n",
    "# paths.extend(synthfols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1.])]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'round'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2072964/1098050754.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestaccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_categorical_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_continuous_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_target_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2072964/881577358.py\u001b[0m in \u001b[0;36mtestaccuracy\u001b[0;34m(model, test_categorical_dataloader, test_continuous_dataloader, test_target_dataloader)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0my_pred_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'round'"
     ]
    }
   ],
   "source": [
    "for p in paths:\n",
    "    \n",
    "#     if os.path.exists(path+'preds/DRO_pred.pt') == False:\n",
    "\n",
    "    train_df = pd.read_csv(path+'train.csv',header=None)\n",
    "\n",
    "    train_df.columns = ['age', 'workclass', 'fnlwgt','education', 'education.num', 'marital.status',\n",
    "           'occupation', 'relationship', 'race', 'sex', 'capital.gain',\n",
    "           'capital.loss', 'hours.per.week', 'native.country','income']\n",
    "\n",
    "    train_df.index = train_df['sex']\n",
    "\n",
    "    train_df = train_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "\n",
    "    test_df = pd.read_csv(path+'test.csv',header=None)\n",
    "\n",
    "    test_df.columns = ['age', 'workclass', 'fnlwgt','education', 'education.num', 'marital.status',\n",
    "           'occupation', 'relationship', 'race', 'sex', 'capital.gain',\n",
    "           'capital.loss', 'hours.per.week', 'native.country','income']\n",
    "\n",
    "    test_df.index = test_df['sex']\n",
    "    \n",
    "    cats = ['workclass', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "    conts = ['age', 'fnlwgt', 'capital.gain', 'capital.loss', 'hours.per.week']\n",
    "    target = 'income'\n",
    "    \n",
    "    df = train_df\n",
    "    \n",
    "    for cont in conts:\n",
    "        df[cont] = df[cont].astype('float64')\n",
    "    \n",
    "    df['workclass'] = df['workclass'].apply(lambda x: x if x not in ['?', 'Without-pay', 'Never-worked'] else 'Without-pay')\n",
    "    \n",
    "    df['marital.status'] = df['marital.status'].apply(lambda x: x if x not in ['Married-AF-spouse', 'Married-spouse-absent'] else 'Married-spouse-absent')\n",
    "    \n",
    "    df['native.country'] = df['native.country'].apply(lambda x: x if x == 'United-States' else 'other')\n",
    "    \n",
    "    df[target] = df[target].astype('category')\n",
    "    \n",
    "    df = df.astype({x: 'category' for x in cats})\n",
    "    for col in cats:\n",
    "        df[col] = df[col].cat.codes.values\n",
    "\n",
    "    df[target] = df[target].cat.codes.values\n",
    "    \n",
    "    for cat in cats:\n",
    "        df[cat] = df[cat].astype('int32')\n",
    "    \n",
    "    df.drop('education', inplace=True, axis=1)\n",
    "    \n",
    "    train_df = df\n",
    "        \n",
    "    df = test_df\n",
    "    \n",
    "    for cont in conts:\n",
    "        df[cont] = df[cont].astype('float64')\n",
    "    \n",
    "    \n",
    "    df['workclass'] = df['workclass'].apply(lambda x: x if x not in ['?', 'Without-pay', 'Never-worked'] else 'Without-pay')\n",
    "    \n",
    "    df['marital.status'] = df['marital.status'].apply(lambda x: x if x not in ['Married-AF-spouse', 'Married-spouse-absent'] else 'Married-spouse-absent')\n",
    "    \n",
    "    df['native.country'] = df['native.country'].apply(lambda x: x if x == 'United-States' else 'other')\n",
    "    \n",
    "    df[target] = df[target].astype('category')\n",
    "    \n",
    "    df = df.astype({x: 'category' for x in cats})\n",
    "    for col in cats:\n",
    "        df[col] = df[col].cat.codes.values\n",
    "\n",
    "    df[target] = df[target].cat.codes.values\n",
    "    \n",
    "    df.drop('education', inplace=True, axis=1)\n",
    "    \n",
    "    test_df = df\n",
    "    \n",
    "    X_train = train_df.drop(['income'],axis=1)\n",
    "    X_test = test_df.drop(['income'],axis=1)\n",
    "    \n",
    "    y_train = train_df['income']\n",
    "    y_test = test_df['income']\n",
    "    \n",
    "    age_max = X_train['age'].max()\n",
    "    age_min = X_train['age'].min()\n",
    "\n",
    "    X_train['age'] = X_train['age'].apply(lambda x: (x - age_min) / (age_max - age_min))\n",
    "    \n",
    "    fnlwgt_max = X_train['fnlwgt'].max()\n",
    "    fnlwgt_min = X_train['fnlwgt'].min()\n",
    "\n",
    "    X_train['fnlwgt'] = X_train['fnlwgt'].apply(lambda x: (x - fnlwgt_min) / (fnlwgt_max - fnlwgt_min))\n",
    "    \n",
    "    capital_max = X_train['capital.gain'].max()\n",
    "    capital_min = X_train['capital.gain'].min()\n",
    "\n",
    "    X_train['capital.gain'] = X_train['capital.gain'].apply(lambda x: (x - capital_min) / (capital_max - capital_min))\n",
    "    \n",
    "    capital_loss_max = X_train['capital.loss'].max()\n",
    "    capital_loss_min = X_train['capital.loss'].min()\n",
    "\n",
    "    X_train['capital.loss'] = X_train['capital.loss'].apply(lambda x: (x - capital_loss_min) / (capital_loss_max - capital_loss_min))\n",
    "    \n",
    "    hours_per_week_max = X_train['hours.per.week'].max()\n",
    "    hours_per_week_min = X_train['hours.per.week'].min()\n",
    "\n",
    "    X_train['hours.per.week'] = X_train['hours.per.week'].apply(lambda x: (x - hours_per_week_min) / (hours_per_week_max - hours_per_week_min))\n",
    "    \n",
    "    X_test['age'] = X_test['age'].apply(lambda x: (x - age_min) / (age_max - age_min))\n",
    "    X_test['fnlwgt'] = X_test['fnlwgt'].apply(lambda x: (x - fnlwgt_min) / (fnlwgt_max - fnlwgt_min))\n",
    "    X_test['capital.gain'] = X_test['capital.gain'].apply(lambda x: (x - capital_min) / (capital_max - capital_min))\n",
    "    X_test['capital.loss'] = X_test['capital.loss'].apply(lambda x: (x - capital_loss_min) / (capital_loss_max - capital_loss_min))\n",
    "    X_test['hours.per.week'] = X_test['hours.per.week'].apply(lambda x: (x - hours_per_week_min) / (hours_per_week_max - hours_per_week_min))\n",
    "    \n",
    "    for cat in cats:\n",
    "        X_train[cat] = X_train[cat].astype('int32')\n",
    "        X_test[cat] = X_test[cat].astype('int32')\n",
    "    \n",
    "    batch_size = 256\n",
    "\n",
    "    train_categorical_dataloader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.Tensor(X_train[cats].to_numpy())\n",
    "        ),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    train_continuous_dataloader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.Tensor(X_train[conts].to_numpy())\n",
    "        ),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    train_target_dataloader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.Tensor(y_train.to_numpy())\n",
    "        ),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_categorical_dataloader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.Tensor(X_test[cats].to_numpy())\n",
    "        ),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    test_continuous_dataloader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.Tensor(X_test[conts].to_numpy())\n",
    "        ),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    test_target_dataloader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.Tensor(y_test.to_numpy())\n",
    "        ),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    embeddings_sizes = []\n",
    "\n",
    "    for cat in cats:\n",
    "        categories = len(df[cat].unique())\n",
    "        embedding_size = (categories + 1) // 2\n",
    "        embeddings_sizes.append((categories, embedding_size))    \n",
    "    \n",
    "    model = Model(embedding_sizes=embeddings_sizes, n_continuous=len(conts))  \n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0006)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    robust_loss = RobustLoss(geometry='chi-square', size=1.0, reg=0.5)\n",
    "    \n",
    "    epochs = 100\n",
    "    losses = []\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i, (x_cat, x_cont, y) in enumerate(zip(train_categorical_dataloader, train_continuous_dataloader, train_target_dataloader)):\n",
    "\n",
    "            # convert list to tensor\n",
    "            y = y[0].float().reshape(-1, 1)\n",
    "\n",
    "            y_pred = model(x_cat, x_cont)\n",
    "            loss = robust_loss(criterion(y_pred, y.flatten().long()))\n",
    "\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        accuracy = testaccuracy(model,test_categorical_dataloader,test_continuous_dataloader,test_target_dataloader)\n",
    "        print('accuracy:', accuracy)\n",
    "        print('best:', best_accuracy)\n",
    "        if accuracy > best_accuracy:\n",
    "            saveModel(model)\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        print(f\"epoch: {epoch+1}      loss: {loss}\")\n",
    "        \n",
    "    finalmodel = Model(embedding_sizes=embeddings_sizes, n_continuous=len(conts))  \n",
    "    finalmodel.load_state_dict(torch.load('DRO_model.pth'))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = None\n",
    "        for i, (x_cat, x_cont, y) in enumerate(zip(test_categorical_dataloader, test_continuous_dataloader, test_target_dataloader)):\n",
    "            y_pred = finalmodel(x_cat, x_cont)\n",
    "        print(y_pred)\n",
    "#         y_pred_class=y_pred.round()\n",
    "#         try:\n",
    "#             os.mkdir(path+'preds/')\n",
    "#         except:\n",
    "#             pass\n",
    "#         torch.save(y_pred_class,path+'preds/DRO_pred.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37]",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
